import os
import io
import json
import base64
import shutil
import numpy as np
import torch
from PIL import Image
from huggingface_hub import hf_hub_download
import runpod

# --- YENƒ∞ EKLENEN IMPORTLAR (Hata √á√∂z√ºc√º) ---
from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection

# Global Deƒüi≈ükenler
MODEL_LOADED = False
model = {}
BASE_REPO = "yisol/IDM-VTON"

# -------------------------------------------------
# 1. DOSYA ƒ∞NDƒ∞RME (Eksik Par√ßa: preprocessor_config.json)
# -------------------------------------------------
def ensure_ckpts():
    print("‚¨áÔ∏è Model dosyalarƒ± indiriliyor (Feature Extractor eklendi)...")
    
    tasks = [
        # --- 1. UNET GARM (StabilityAI -> Yerel unet_garm) ---
        {
            "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
            "remote": "unet/config.json",
            "locals": ["unet_garm/config.json"]
        },
        {
            "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
            "remote": "unet/diffusion_pytorch_model.fp16.safetensors",
            "locals": ["unet_garm/diffusion_pytorch_model.safetensors"] 
        },

        # --- 2. IMAGE ENCODER & FEATURE EXTRACTOR (Laion) ---
        # HATA BURADAYDI: preprocessor_config.json eksikti. Ekledik.
        {
            "repo_id": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
            "remote": "config.json",
            "locals": ["image_encoder/config.json"]
        },
        {
            "repo_id": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
            "remote": "model.safetensors", 
            "locals": ["image_encoder/model.safetensors"]
        },
        {
            "repo_id": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
            "remote": "preprocessor_config.json",   # <-- YENƒ∞ EKLENEN KRƒ∞Tƒ∞K DOSYA
            "locals": ["image_encoder/preprocessor_config.json"]
        },

        # --- 3. IP ADAPTER (h94) ---
        {
            "repo_id": "h94/IP-Adapter",
            "remote": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin",
            "locals": ["ip_adapter/adapter_model.bin"]
        },

        # --- 4. OPENPOSE (Yisol) ---
        {
            "repo_id": "yisol/IDM-VTON",
            "remote": "openpose/ckpts/body_pose_model.pth",
            "locals": [
                "ckpt/openpose/ckpts/body_pose_model.pth",
                "preprocess/openpose/ckpts/body_pose_model.pth"
            ]
        },
        
        # --- 5. DENSEPOSE & HUMANPARSING (Yisol) ---
        {
            "repo_id": "yisol/IDM-VTON",
            "remote": "densepose/model_final_162be9.pkl",
            "locals": ["ckpt/densepose/densepose_model.pkl"]
        },
        {
            "repo_id": "yisol/IDM-VTON",
            "remote": "humanparsing/parsing_atr.onnx",
            "locals": ["ckpt/humanparsing/parsing_atr.onnx"]
        },
        {
            "repo_id": "yisol/IDM-VTON",
            "remote": "humanparsing/parsing_lip.onnx",
            "locals": ["ckpt/humanparsing/parsing_lip.onnx"]
        }
    ]

    for task in tasks:
        repo_id = task["repo_id"]
        remote_path = task["remote"]
        local_paths = task["locals"]

        try:
            print(f"‚è≥ ƒ∞ndiriliyor ({repo_id}): {remote_path}")
            downloaded_file_path = hf_hub_download(
                repo_id=repo_id,
                filename=remote_path
            )
        except Exception as e:
            print(f"‚ùå HATA: {remote_path} indirilemedi! Detay: {e}")
            raise e

        for local_path in local_paths:
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            shutil.copy(downloaded_file_path, local_path)
            print(f"‚úÖ Hazƒ±r: {local_path}")

# -------------------------------------------------
# 2. MODEL LOAD (Pipe Feature Extractor Fix)
# -------------------------------------------------
def load_model():
    global MODEL_LOADED, model

    if MODEL_LOADED:
        return model

    #ensure_ckpts()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    from preprocess.humanparsing.run_parsing import Parsing
    from preprocess.openpose.run_openpose import OpenPose
    from src.tryon_pipeline import StableDiffusionXLInpaintPipeline
    from src.unet_hacked_tryon import UNet2DConditionModel
    from src.unet_hacked_garmnet import UNet2DConditionModel as UNetGarm

    print("üîÑ Modeller y√ºkleniyor...")
    
    parsing = Parsing(0)
    openpose = OpenPose(0)

    # 1. Image Encoder ve Feature Extractor'ƒ± yerel klas√∂rden elle y√ºkl√ºyoruz
    # B√∂ylece pipeline Yisol'un bo≈ü reposuna bakmak zorunda kalmƒ±yor.
    print("üîÑ Image Encoder & Feature Extractor y√ºkleniyor...")
    image_encoder = CLIPVisionModelWithProjection.from_pretrained(
        "image_encoder", 
        torch_dtype=torch.float16
    )
    feature_extractor = CLIPImageProcessor.from_pretrained(
        "image_encoder",
        torch_dtype=torch.float16
    )

    # 2. Ana UNet
    unet = UNet2DConditionModel.from_pretrained(
        BASE_REPO,
        subfolder="unet",
        torch_dtype=torch.float16
    )

    # 3. GarmNet (Yerel unet_garm klas√∂r√ºnden)
    print("üîÑ UNet Garm y√ºkleniyor...")
    unet_garm = UNetGarm.from_pretrained(
        "unet_garm",
        torch_dtype=torch.float16,
        use_safetensors=True
    )

    # 4. Pipeline (Elle y√ºklediƒüimiz par√ßalarƒ± buraya veriyoruz)
    print("üîÑ Pipeline birle≈ütiriliyor...")
    pipe = StableDiffusionXLInpaintPipeline.from_pretrained(
        BASE_REPO,
        unet=unet,
        image_encoder=image_encoder,        # <-- ELLE VERDƒ∞K
        feature_extractor=feature_extractor, # <-- ELLE VERDƒ∞K (Hata √á√∂z√ºc√º)
        torch_dtype=torch.float16,
        use_safetensors=True,
    ).to(device)

    pipe.unet_garm = unet_garm

    model = {
        "pipe": pipe,
        "parsing": parsing,
        "openpose": openpose,
        "device": device
    }

    MODEL_LOADED = True
    print("‚úÖ M√ºkemmel! Sistem hazƒ±r.")
    return model

# -------------------------------------------------
# HELPERS
# -------------------------------------------------
def b64_to_img(b64):
    if "," in b64:
        b64 = b64.split(",")[1]
    return Image.open(io.BytesIO(base64.b64decode(b64))).convert("RGB")

def img_to_b64(img):
    buf = io.BytesIO()
    img.save(buf, format="JPEG", quality=95)
    return base64.b64encode(buf.getvalue()).decode()

# -------------------------------------------------
# HANDLER (RGB -> BGR FIX)
# -------------------------------------------------
def handler(job):
    data = job["input"]

    # 1. Resimleri Y√ºkle (PIL = RGB formatƒ±nda gelir)
    human = b64_to_img(data["human_image"]).resize((768, 1024))
    garment = b64_to_img(data["garment_image"]).resize((768, 1024))
    
    if garment.mode == 'RGBA':
        garment = garment.convert('RGB')

    steps = data.get("steps", 30)
    seed = data.get("seed", 42)

    mdl = load_model()

    with torch.no_grad():
        # --- KRƒ∞Tƒ∞K D√úZELTME BURASI ---
        # 1. √ñnce resmi Numpy dizisine √ßevir (Hala RGB)
        human_np_rgb = np.array(human)
        
        # 2. RGB'yi BGR'ye √ßevir (OpenPose BGR sever!)
        # [:, :, ::-1] komutu renk kanallarƒ±nƒ± tersten dizer (Red <-> Blue yer deƒüi≈üir)
        human_np_bgr = human_np_rgb[:, :, ::-1].copy()

        # --- PARSING (PIL / RGB ƒ∞ster) ---
        print("Run Parsing (PIL)...")
        try:
            parsing = mdl["parsing"](human)
        except Exception as e:
            print(f"‚ö†Ô∏è Parsing uyarƒ±sƒ±: {e}")
            parsing = Image.new("RGB", human.size, (0,0,0))

        # --- OPENPOSE (Numpy / BGR ƒ∞ster) ---
        print("Run OpenPose (BGR Numpy)...")
        try:
            # Artƒ±k BGR formatƒ±nda veriyoruz, ten rengini doƒüru anlayacak
            pose = mdl["openpose"](human_np_bgr)
        except ValueError:
            print("‚ö†Ô∏è UYARI: OpenPose yine de insan bulamadƒ±! (Bo≈ü pose kullanƒ±lƒ±yor)")
            pose = Image.new("RGB", human.size, (0,0,0))
        except Exception as e:
            print(f"‚ö†Ô∏è OpenPose bilinmeyen hata: {e}")
            pose = Image.new("RGB", human.size, (0,0,0))

        # --- PIPELINE ---
        print("Run Pipeline...")
        result = mdl["pipe"](
            prompt="clothes",
            image=human,
            garment=garment,
            pose=pose,
            num_inference_steps=steps,
            guidance_scale=2.0,
            generator=torch.Generator(mdl["device"]).manual_seed(seed),
            height=1024,
            width=768
        ).images[0]

    return {"output": img_to_b64(result)}

runpod.serverless.start({"handler": handler})
